import argparsefrom functools import partialfrom stable_baselines3 import PPOfrom environment.agent import (    OpponentsCfg,    SaveHandler,    SaveHandlerMode,    SelfPlayRandom,    TrainLogging,    CameraResolution,    train,    ConstantAgent,    BasedAgent,)from user.train_agent import CustomAgent, MLPExtractor, gen_reward_managerdef opponent_mix(progress: float, selfplay_handler: SelfPlayRandom):    """    Curriculum schedule for opponent probabilities.    progress: 0.0 -> 1.0 across the 80M plan    Returns a dict suitable for OpponentsCfg(opponents=...)    """    if progress < 0.10:        # Warm-up: learn to move/attack/recover        return {            'constant_agent': (2.0, partial(ConstantAgent)),            'based_agent': (2.0, partial(BasedAgent)),            'self_play': (1.0, selfplay_handler),        }    elif progress < 0.30:        # Start mixing in self-play        return {            'constant_agent': (1.0, partial(ConstantAgent)),            'based_agent': (2.0, partial(BasedAgent)),            'self_play': (4.0, selfplay_handler),        }    elif progress < 0.70:        # Self-play heavy        return {            'constant_agent': (0.5, partial(ConstantAgent)),            'based_agent': (1.5, partial(BasedAgent)),            'self_play': (6.0, selfplay_handler),        }    else:        # Mostly self-play for polish        return {            'constant_agent': (0.2, partial(ConstantAgent)),            'based_agent': (0.8, partial(BasedAgent)),            'self_play': (8.0, selfplay_handler),        }def main():    parser = argparse.ArgumentParser(description="Automate AI^2 training to large timesteps.")    parser.add_argument("--run-name", default="comp_run_80m", help="Folder name under checkpoints/")    parser.add_argument("--save-path", default="checkpoints", help="Base path for checkpoints")    parser.add_argument("--total-steps", type=int, default=80_000_000, help="Total training timesteps")    parser.add_argument("--segment-steps", type=int, default=2_000_000, help="Steps per training segment")    parser.add_argument("--save-freq", type=int, default=50_000, help="Checkpoint frequency")    parser.add_argument("--max-saved", type=int, default=50, help="Max number of checkpoints to keep (-1 = unlimited)")    parser.add_argument("--resume", action="store_true", help="Resume from existing run directory (RESUME mode)")    parser.add_argument("--log", choices=["none", "file", "plot"], default="none", help="Training logs level")    args = parser.parse_args()    # Create agent    my_agent = CustomAgent(sb3_class=PPO, extractor=MLPExtractor)    # Save handler (RESUME to keep pool for self-play)    mode = SaveHandlerMode.RESUME if args.resume else SaveHandlerMode.FORCE    save_handler = SaveHandler(        agent=my_agent,        save_freq=args.save_freq,        max_saved=args.max_saved,        save_path=args.save_path,        run_name=args.run_name,        mode=mode,    )    # Train in segments so we can adjust curriculum and resume safely    steps_done = 0    while steps_done < args.total_steps:        seg_steps = min(args.segment_steps, args.total_steps - steps_done)        progress = steps_done / max(args.total_steps, 1)        # Build curriculum opponents for this segment        selfplay_handler = SelfPlayRandom(partial(type(my_agent)))        opp_spec = opponent_mix(progress, selfplay_handler)        opponent_cfg = OpponentsCfg(opponents=opp_spec)        # Reward manager (skill shaping defined in user/train_agent.py)        reward_manager = gen_reward_manager()        # Map log arg        log_map = {            "none": TrainLogging.NONE,            "file": TrainLogging.TO_FILE,            "plot": TrainLogging.PLOT,        }        train(            my_agent,            reward_manager,            save_handler,            opponent_cfg,            CameraResolution.LOW,            train_timesteps=seg_steps,            train_logging=log_map[args.log],        )        steps_done += seg_stepsif __name__ == "__main__":    main()